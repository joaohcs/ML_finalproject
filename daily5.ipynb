{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perguntas sobre Reinforcement Learning com contexto do Sutton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lendo a chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98a62383-2f1c-4112-91a4-4ced24915603'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando o livro do Sutton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutton - RL\n",
    "loader = PyPDFLoader('../PDF/rl4rs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl4rs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Wang, et al.\\nTable 1: A comparison between RL4RS and other resources.\\nDataset Opensource Others\\nResourceArtifical\\ndatasetSemi-simulated\\ndatasetReal\\ndatasetCodeRaw logged\\ndataSimulation\\nenvironmentOffline policy\\nlearningOffline policy\\nevaluation\\nRecoGym[32] ✓× × ✓× ✓× ×\\nRecsim[20] ✓× × ✓× ✓× ×\\nVirtual-Taobao[36] × × ✓ ✓ × ✓× ×\\nTop-k Off-policy[3] ✓× × ✓× × ✓×\\nSlateQ[21] ✓× × ✓× × × ×\\nAdversarial Model[4] × ✓× ✓× × × ×\\nList-wise[43] × ✓× ✓× × × ×\\nModel-based RS[1] ✓ ✓ × ✓× × ✓×\\nOurs × × ✓ ✓ ✓ ✓ ✓ ✓\\nitself. If an item is surrounded by similar but expensive items, the\\nconversion rate increases, known as the decoy effect [ 19]. However,\\nthe possible combinations of all the items can be billions, which is\\nan NP-hard problem and less explored in supervised learning (SL).\\nTo deal with these challenges, recent researchers resort to adopt-\\ning reinforcement learning for recommendations, in which the\\nrecommendation process is formulated as a sequential interaction\\nbetween the user (environment) and the recommendation agent\\n(RL agent), as illustrated in Figure 2. Reinforcement learning is\\na promising direction since the RL paradigm is inherently suit-\\nable for tackling multi-step decision-making problems, optimizing\\nlong-term user satisfaction directly, and exploring the combina-\\ntion spaces efficiently - but there remain two problems in recent\\nresearch.\\nThe first problem is the lack of real-world datasets for RL-based\\nRS problems. There are mainly two alternatives, one is artificial\\ndatasets, such as RecoGym [ 32] and RECSIM [ 20]. The main dis-\\nadvantage is that they are not the real feedback of users in real\\napplications. Another is semi-simulated datasets, i.e., traditional\\nRS datasets (e.g., MovieLens) that transformed to RL data format.\\nTake MovieLens dataset as an example, to meet the requirements\\nof RL data format, The Adversarial User Model [ 4] introduces ex-\\nternal movie information and assumes the context of user’s choice\\nas the movies released within a month. The maximal size of each\\ndisplayed set is set as 40. The main disadvantage of semi-simulated\\ndatasets is that many forced data transformations are unreasonable.\\nThe second problem is the lack of unbiased evaluation methods.\\nIn the current research, there are mainly two kinds of evaluation\\nindicators: traditional recommendation indicators (Recall Rate, Ac-\\ncuracy, etc.) and pure reinforcement learning indicators (e.g., Cu-\\nmulative Rewards). However, the former are short-term evaluation\\nindicators, and the latter highly depend on the accuracy of the sim-\\nulation environment. The bias of policy evaluation also comes from\\n\"extrapolation error\", a phenomenon in which unseen state-action\\npairs are erroneously estimated to have unrealistic values. In this\\npaper, we propose a new evaluation framework and explore two\\nother recently developed methods to tackle \"extrapolation error\",\\ncounterfactual policy evaluation and batch RL.\\nWith these in mind, we introduce RL4RS - an open-source dataset\\nfor RL-based RS developed and deployed at Netease. RL4RS is builtin Python and uses TensorFlow for modeling and training. It aims\\nto fill the rapidly-growing need for RL systems that are tailored\\nto work on novel recommendation scenarios. It consists of (1) two\\nlarge-scale raw logged data, reproducible simulation environments,\\nand related RL baselines. (2) data understanding tools for testing the\\nproper use of RL, and a systematic evaluation framework, including\\nevaluation of environment simulation, policy evaluation on simu-\\nlation environments, and counterfactual policy evaluation. (3) the\\nseparated data before and after reinforcement learning deployment\\nfor each dataset, e.g., Slate-SL and Slate-RL. Based on them, we are\\nable to evaluate the effectiveness of different batch RL algorithms\\nand measure the extent of extrapolation error, by taking Slate-SL\\nas train set and Slate-RL as test set.\\nfrom%&((&,*+,…,*-)to%&+1((&,*+,…,*-21)\\n1\\nFigure 2: The user-agent interaction in the Markov Decision\\nProcess (MDP) of RL-based recommender systems.\\n2 IMPACT\\nIn this section, we assess the benefits of the RL4RS resource in rela-\\ntion to existing resources for evaluating RL-based RS. We collect\\nall relevant works that have been open-sourced at present, includ-\\ning dataset/environment resources (RecoGym1[32], Recsim2[20],\\nand Virtual Taobao3[36]) and representative method/algorithm re-\\nsources (Top-k off-policy4[3], SlateQ5[21], Adversarial User Model6[4],\\n1https://github.com/criteo-research/reco-gym\\n2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel', metadata={'source': '../PDF/rl4rs.pdf', 'page': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl4rs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quebrando em mais pedaços (`Chunks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definições dos chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(rl4rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings de cada documento + busca semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,\n",
    "              environment=PINECONE_API_ENV)\n",
    "index_name = 'rl4rs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passa todos os embeddings e guarda no index criado na conta em Pinecone\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lançando a query e vendo os resultados mais similares\n",
    "query = 'What has been the economic relevance of recommender systems in industry?'\n",
    "docs = docsearch.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation'),\n",
       " Document(page_content='sequential recommendation methods) is well enough and shows\\nthe shortcomings of benchmarking on datasets collected from tra-\\nditional scenarios. In practice, the data understanding tool may\\nhelp us understand the dataset in the early stages, catch improperly\\ndefined RL problems, and lead to a better problem definition (e.g.,\\nreward definition).\\nTable 3: The comparison of average scores of decode se-\\nquence between different datasets.\\nScore of 5% 20% greedy hot 5% hot 20%\\nRecSys15 1.00 0.53 1.26 0.81 0.42\\nMovieLens 1.00 0.64 0.99 0.98 0.62\\nLast.fm 1.00 0.47 1.09 0.90 0.49\\nCIKMCup2016 1.00 0.30 4.50 0.99 0.28\\nRL4RS-Slate 1.00 0.76 0.62 0.01 0.01\\n5 EVALUATION FRAMEWORK\\nUnlike academic research, in real applications, it is usually rare to\\nhave access to a perfect simulator. A policy should be well evaluated\\nbefore deployment because policy deployment affects the real world\\nand can be costly. In applied settings, policy evaluation is not a'),\n",
       " Document(page_content='must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n©2018 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nFigure 1: Several novel recommendation scenarios.\\n1 INTRODUCTION\\nIn 2022, retail e-commerce sales worldwide amounted to 5.7 trillion\\nUS dollars, and e-retail revenues are projected to grow to 6.3 trillion\\nUS dollars in 2023. Such rapid growth promises an excellent future\\nfor the worldwide e-commerce industry, signifying a strong market\\nand increased customer demand. Besides the massive increment\\nof traffic volume, there has been a rapid growth of various rec-\\nommendation scenarios, including slate recommendation (lists of'),\n",
       " Document(page_content='RL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n•Information systems →Recommender systems ;•Comput-\\ning methodologies →Reinforcement learning .\\nKEYWORDS')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consultando os documentos para obtermos nossa resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The economic relevance of recommender systems in industry has been significant, with the e-commerce industry projected to grow to 6.3 trillion US dollars in 2023. This growth is driven in part by the rapid increase in various recommendation scenarios, such as slate recommendation and sequential recommendation, which aim to maximize user satisfaction and increase sales. However, there are challenges in implementing these systems, and the RL4RS (Reinforcement Learning for Recommender Systems) resource has been developed to address these challenges and contribute to research in applied reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(input_documents=docs, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chat implementando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Antes de implementar RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='Você é um bot assistente gentil e prestativo.'),\n",
    "    HumanMessage(content='Olá, como você está hoje?'),\n",
    "    AIMessage(content='Eu estou bem, obrigado por perguntar. Como eu possso te ajudar?'),\n",
    "    HumanMessage(content='Eu gostaria de saber mais sobre sistemas de recomendação baseados em reinforcement learning.')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Claro! Sistemas de recomendação baseados em reinforcement learning (aprendizado por reforço) são uma abordagem que utiliza técnicas de machine learning para recomendar itens ou ações personalizadas aos usuários.\\n\\nO aprendizado por reforço é uma técnica de machine learning onde um agente aprende a tomar decisões em um ambiente para maximizar uma recompensa. No caso de sistemas de recomendação, o agente é o sistema de recomendação em si e o ambiente é o conjunto de usuários e itens disponíveis para recomendação.\\n\\nNesse tipo de sistema, o sistema de recomendação recebe informações sobre as preferências e ações dos usuários, bem como o feedback sobre as recomendações anteriores. Com base nessas informações, ele aprende a tomar decisões sobre quais itens recomendar para cada usuário da maneira mais efetiva possível.\\n\\nO objetivo é encontrar um equilíbrio entre explorar novos itens desconhecidos para os usuários e explorar itens já conhecidos e mais prováveis de serem aceitos. O sistema de recomendação utiliza algoritmos de reinforcement learning para aprender a fazer essas escolhas de forma otimizada, levando em consideração as recompensas recebidas pelos usuários.\\n\\nEsses sistemas são especialmente úteis em ambientes onde as preferências dos usuários podem mudar ao longo do tempo ou onde há uma ampla gama de itens disponíveis para recomendação. Eles podem ser aplicados em diferentes contextos, como recomendação de produtos, personalização de conteúdo, recomendação de músicas, entre outros.\\n\\nNo entanto, é importante notar que sistemas de recomendação baseados em reinforcement learning podem ser complexos de implementar e requerem grande quantidade de dados para aprendizado efetivo. Além disso, a ética e a privacidade dos usuários devem ser consideradas ao aplicar essas técnicas.\\n\\nEspero que essa explicação tenha sido útil! Se você tiver mais perguntas, estou aqui para ajudar.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Sistemas de recomendação baseados em reinforcement learning (aprendizado por reforço) são uma abordagem que utiliza técnicas de machine learning para recomendar itens ou ações personalizadas aos usuários.\n",
      "\n",
      "O aprendizado por reforço é uma técnica de machine learning onde um agente aprende a tomar decisões em um ambiente para maximizar uma recompensa. No caso de sistemas de recomendação, o agente é o sistema de recomendação em si e o ambiente é o conjunto de usuários e itens disponíveis para recomendação.\n",
      "\n",
      "Nesse tipo de sistema, o sistema de recomendação recebe informações sobre as preferências e ações dos usuários, bem como o feedback sobre as recomendações anteriores. Com base nessas informações, ele aprende a tomar decisões sobre quais itens recomendar para cada usuário da maneira mais efetiva possível.\n",
      "\n",
      "O objetivo é encontrar um equilíbrio entre explorar novos itens desconhecidos para os usuários e explorar itens já conhecidos e mais prováveis de serem aceitos. O sistema de recomendação utiliza algoritmos de reinforcement learning para aprender a fazer essas escolhas de forma otimizada, levando em consideração as recompensas recebidas pelos usuários.\n",
      "\n",
      "Esses sistemas são especialmente úteis em ambientes onde as preferências dos usuários podem mudar ao longo do tempo ou onde há uma ampla gama de itens disponíveis para recomendação. Eles podem ser aplicados em diferentes contextos, como recomendação de produtos, personalização de conteúdo, recomendação de músicas, entre outros.\n",
      "\n",
      "No entanto, é importante notar que sistemas de recomendação baseados em reinforcement learning podem ser complexos de implementar e requerem grande quantidade de dados para aprendizado efetivo. Além disso, a ética e a privacidade dos usuários devem ser consideradas ao aplicar essas técnicas.\n",
      "\n",
      "Espero que essa explicação tenha sido útil! Se você tiver mais perguntas, estou aqui para ajudar.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peço desculpas, mas não estou familiarizado com um framework chamado RL4RS. Não consigo fornecer informações detalhadas sobre isso. No entanto, fique à vontade para perguntar sobre outros frameworks ou detalhes específicos sobre reinforcement learning ou sistemas de recomendação. Estou aqui para ajudar!\n"
     ]
    }
   ],
   "source": [
    "# Vamos adicionar a última resposta do chat às mensagens\n",
    "messages.append(res)\n",
    "\n",
    "# criando um novo user prompt\n",
    "prompt = HumanMessage(\n",
    "    content='Você conhece o framework RL4RS?'\n",
    ")\n",
    "\n",
    "# adicionando às mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Inserindo contexto (conceito de RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = docsearch.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta não estiver relacionada ao contexto, esqueça o contexto dado e responda como você normalmente responderia, pois você ainda é um assistente prestativo. E se apesar da pergunta não estiver relacionada você não saber a resposta, apenas diga que não sabe como normalmente você faria.\n",
    "\n",
    "    Contextos:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, agora, na resposta dada após a inserção de contexto utilizando o próprio artigo do framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim, eu conheço o framework RL4RS. O RL4RS (Reinforcement Learning for Recommender Systems) é um recurso desenvolvido pela Netease, que consiste em um conjunto de dados de código aberto para sistemas de recomendação baseados em reinforcement learning. Ele inclui conjuntos de dados em larga escala, ambientes de simulação reprodutíveis, baselines avançados de RL relacionados e ferramentas de compreensão de dados.\n",
      "\n",
      "O RL4RS também oferece um framework de avaliação sistemática, que inclui avaliação da simulação do ambiente, avaliação do ambiente de simulação e avaliação da política contrafactual. Ele foi construído em Python e utiliza TensorFlow para modelagem e treinamento.\n",
      "\n",
      "O objetivo do RL4RS é atender à crescente demanda por sistemas de RL adaptados a cenários de recomendação inovadores. Ele fornece uma plataforma para o desenvolvimento e avaliação de sistemas de recomendação baseados em RL.\n",
      "\n",
      "Você pode encontrar o RL4RS no repositório do GitHub da Netease, no seguinte link: https://github.com/fuxiAIlab/RL4RS.\n",
      "\n",
      "Espero que isso esclareça sua dúvida! Se você tiver mais perguntas, estou aqui para ajudar.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt('Você conhece o framework RL4RS?')\n",
    ")\n",
    "\n",
    "# adicionando às mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Estou aqui para ajudar. Por favor, diga-me qual é sua pergunta ou problema específico, e eu farei o possível para fornecer orientação ou suporte.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt('Você poderia me ajudar?')\n",
    ")\n",
    "\n",
    "# adicionando às mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Gerando chat com contexto inserido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = [SystemMessage(content='Você é um assistente gentil e prestativo. Busque responder as perguntas quando conseguir!')]\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=augment_prompt(message)))\n",
    "    gpt_response = chat(history_langchain_format)\n",
    "    return gpt_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(predict).launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Assistente de voz com contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Você é um assistente gentil e prestativo e que responde no idioma do usuário.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta não estiver relacionada ao contexto, esqueça o contexto dado e responda como você normalmente responderia, pois você ainda é um assistente prestativo. E se apesar da pergunta não estiver relacionada você não saber a resposta, apenas diga que não sabe como normalmente você faria.\\n\\n    Contextos:\\n    2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel\\n2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel\\nSection 3), and are updated weekly. We employ metrics includ-\\ning IPV (Individual Page View), CVR (Conversion Rate), and GMV\\n(Gross Merchandise Volume).\\nDuring the A/B test in one month, the averaged results are pre-\\nsented in Table 9. On each day of the test, the PPO algorithm can\\nlead to more GMV than the DIEN, A2C, and DQN (an increase of\\n23.9%,8.4%,𝑎𝑛𝑑 12.3%, respectively). Compared with the greedy rec-\\nommendation strategy, which recommends only the items that users\\nare interested in at each step, RL methods can make full use of the\\nmulti-step decision-making characteristics of this scenario, and min-\\nimize the loss of CVR while mixing the recommended items users\\nare interested in and other items. In addition, the performance rank-\\ning of online deployment (PPO>A2C>DQN>SL) is consistent with\\nthe ranking in the simulation environment (PPO>A2C>DQN>SL).\\nIt shows that offline experiments can replace live experiments to\\na certain extent. Here, we promise that we will help researchers\\n\\n    Query: Olá, você poderia me ajudar?\\n'}]\n",
      "[{'role': 'system', 'content': 'Você é um assistente gentil e prestativo e que responde no idioma do usuário.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta não estiver relacionada ao contexto, esqueça o contexto dado e responda como você normalmente responderia, pois você ainda é um assistente prestativo. E se apesar da pergunta não estiver relacionada você não saber a resposta, apenas diga que não sabe como normalmente você faria.\\n\\n    Contextos:\\n    ommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation\\nommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation\\nRL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n•Information systems →Recommender systems ;•Comput-\\ning methodologies →Reinforcement learning .\\nKEYWORDS\\n\\n    Query: Me dê uma breve explicação sobre sistemas de recomendação.\\n'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\applications.py\", line 116, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 746, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\responses.py\", line 340, in __call__\n",
      "    await send(\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 512, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\asyncio\\proactor_events.py\", line 162, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\applications.py\", line 116, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 746, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\responses.py\", line 340, in __call__\n",
      "    await send(\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 512, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Você é um assistente gentil e prestativo e que responde no idioma do usuário.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta não estiver relacionada ao contexto, esqueça o contexto dado e responda como você normalmente responderia, pois você ainda é um assistente prestativo. E se apesar da pergunta não estiver relacionada você não saber a resposta, apenas diga que não sabe como normalmente você faria.\\n\\n    Contextos:\\n    RL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n•Information systems →Recommender systems ;•Comput-\\ning methodologies →Reinforcement learning .\\nKEYWORDS\\nRL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n•Information systems →Recommender systems ;•Comput-\\ning methodologies →Reinforcement learning .\\nKEYWORDS\\nRL4RS: A Real-World Dataset for Reinforcement\\nLearning based Recommender System\\nKai Wang1, Zhene Zou1, Minghao Zhao1, Qilin Deng1, Yue Shang1, Yile Liang1,\\nRunze Wu1, Xudong Shen1, Tangjie Lyu1, Changjie Fan1\\n1Fuxi AI Lab, NetEase Inc., Hangzhou, China\\n{wangkai02,zouzhene, zhaominghao,dengqilin,shangyue,liangyile,\\nwurunze1,hzshenxudong ,hzlvtangjie,fanchangjie}\\n@corp.netease.com\\nABSTRACT\\nReinforcement learning based recommender systems (RL-based\\nRS) aim at learning a good policy from a batch of collected data,\\nby casting recommendations to multi-step decision-making tasks.\\nHowever, current RL-based RS research commonly has a large real-\\nity gap. In this paper, we introduce the first open-source real-world\\ndataset, RL4RS, hoping to replace the artificial datasets and semi-\\nsimulated RS datasets previous studies used due to the resource\\nlimitation of the RL-based RS domain. Unlike academic RL research,\\nRL-based RS suffers from the difficulties of being well-validated\\n\\n    Query: Você conhece o framework RL4RS, que é a Real World Dataset for Reforcement Learning Based Recommender System?\\n'}]\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def transcribe2(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    sf.write(\"output_audio.wav\", y, sr, format='WAV', subtype='PCM_16')\n",
    "\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=open('output_audio.wav', 'rb'),\n",
    "        response_format='text'\n",
    "    )\n",
    "    return transcript\n",
    "\n",
    "def voice_chat(user_voice):\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente gentil e prestativo e que responde no idioma do usuário.\"},\n",
    "    ]       \n",
    "    \n",
    "    user_message = transcribe2(user_voice)\n",
    "\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": augment_prompt(user_message)},\n",
    "    )\n",
    "\n",
    "    print(messages)\n",
    "\n",
    "    chat = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=messages\n",
    "    )\n",
    "    \n",
    "    reply = chat.choices[0].message.content\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=reply\n",
    "    )\n",
    "    response.stream_to_file('output.mp3')\n",
    "\n",
    "    return (reply, 'output.mp3')\n",
    "\n",
    "\n",
    "text_reply = gr.Textbox(label=\"ChatGPT Text\")\n",
    "voice_reply = gr.Audio('output.mp3')\n",
    "\n",
    "demo = gr.Interface(\n",
    "    voice_chat,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    outputs=[text_reply, voice_reply],\n",
    "    title = 'AI Voice Assistant with ChatGPT AI'\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
