{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perguntas sobre Reinforcement Learning com contexto do Sutton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lendo a chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98a62383-2f1c-4112-91a4-4ced24915603'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando o livro do Sutton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutton - RL\n",
    "loader = PyPDFLoader('../PDF/rl4rs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl4rs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Wang, et al.\\nTable 1: A comparison between RL4RS and other resources.\\nDataset Opensource Others\\nResourceArtifical\\ndatasetSemi-simulated\\ndatasetReal\\ndatasetCodeRaw logged\\ndataSimulation\\nenvironmentOffline policy\\nlearningOffline policy\\nevaluation\\nRecoGym[32] ‚úì√ó √ó ‚úì√ó ‚úì√ó √ó\\nRecsim[20] ‚úì√ó √ó ‚úì√ó ‚úì√ó √ó\\nVirtual-Taobao[36] √ó √ó ‚úì ‚úì √ó ‚úì√ó √ó\\nTop-k Off-policy[3] ‚úì√ó √ó ‚úì√ó √ó ‚úì√ó\\nSlateQ[21] ‚úì√ó √ó ‚úì√ó √ó √ó √ó\\nAdversarial Model[4] √ó ‚úì√ó ‚úì√ó √ó √ó √ó\\nList-wise[43] √ó ‚úì√ó ‚úì√ó √ó √ó √ó\\nModel-based RS[1] ‚úì ‚úì √ó ‚úì√ó √ó ‚úì√ó\\nOurs √ó √ó ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\\nitself. If an item is surrounded by similar but expensive items, the\\nconversion rate increases, known as the decoy effect [ 19]. However,\\nthe possible combinations of all the items can be billions, which is\\nan NP-hard problem and less explored in supervised learning (SL).\\nTo deal with these challenges, recent researchers resort to adopt-\\ning reinforcement learning for recommendations, in which the\\nrecommendation process is formulated as a sequential interaction\\nbetween the user (environment) and the recommendation agent\\n(RL agent), as illustrated in Figure 2. Reinforcement learning is\\na promising direction since the RL paradigm is inherently suit-\\nable for tackling multi-step decision-making problems, optimizing\\nlong-term user satisfaction directly, and exploring the combina-\\ntion spaces efficiently - but there remain two problems in recent\\nresearch.\\nThe first problem is the lack of real-world datasets for RL-based\\nRS problems. There are mainly two alternatives, one is artificial\\ndatasets, such as RecoGym [ 32] and RECSIM [ 20]. The main dis-\\nadvantage is that they are not the real feedback of users in real\\napplications. Another is semi-simulated datasets, i.e., traditional\\nRS datasets (e.g., MovieLens) that transformed to RL data format.\\nTake MovieLens dataset as an example, to meet the requirements\\nof RL data format, The Adversarial User Model [ 4] introduces ex-\\nternal movie information and assumes the context of user‚Äôs choice\\nas the movies released within a month. The maximal size of each\\ndisplayed set is set as 40. The main disadvantage of semi-simulated\\ndatasets is that many forced data transformations are unreasonable.\\nThe second problem is the lack of unbiased evaluation methods.\\nIn the current research, there are mainly two kinds of evaluation\\nindicators: traditional recommendation indicators (Recall Rate, Ac-\\ncuracy, etc.) and pure reinforcement learning indicators (e.g., Cu-\\nmulative Rewards). However, the former are short-term evaluation\\nindicators, and the latter highly depend on the accuracy of the sim-\\nulation environment. The bias of policy evaluation also comes from\\n\"extrapolation error\", a phenomenon in which unseen state-action\\npairs are erroneously estimated to have unrealistic values. In this\\npaper, we propose a new evaluation framework and explore two\\nother recently developed methods to tackle \"extrapolation error\",\\ncounterfactual policy evaluation and batch RL.\\nWith these in mind, we introduce RL4RS - an open-source dataset\\nfor RL-based RS developed and deployed at Netease. RL4RS is builtin Python and uses TensorFlow for modeling and training. It aims\\nto fill the rapidly-growing need for RL systems that are tailored\\nto work on novel recommendation scenarios. It consists of (1) two\\nlarge-scale raw logged data, reproducible simulation environments,\\nand related RL baselines. (2) data understanding tools for testing the\\nproper use of RL, and a systematic evaluation framework, including\\nevaluation of environment simulation, policy evaluation on simu-\\nlation environments, and counterfactual policy evaluation. (3) the\\nseparated data before and after reinforcement learning deployment\\nfor each dataset, e.g., Slate-SL and Slate-RL. Based on them, we are\\nable to evaluate the effectiveness of different batch RL algorithms\\nand measure the extent of extrapolation error, by taking Slate-SL\\nas train set and Slate-RL as test set.\\nfrom%&((&,*+,‚Ä¶,*-)to%&+1((&,*+,‚Ä¶,*-21)\\n1\\nFigure 2: The user-agent interaction in the Markov Decision\\nProcess (MDP) of RL-based recommender systems.\\n2 IMPACT\\nIn this section, we assess the benefits of the RL4RS resource in rela-\\ntion to existing resources for evaluating RL-based RS. We collect\\nall relevant works that have been open-sourced at present, includ-\\ning dataset/environment resources (RecoGym1[32], Recsim2[20],\\nand Virtual Taobao3[36]) and representative method/algorithm re-\\nsources (Top-k off-policy4[3], SlateQ5[21], Adversarial User Model6[4],\\n1https://github.com/criteo-research/reco-gym\\n2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel', metadata={'source': '../PDF/rl4rs.pdf', 'page': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl4rs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quebrando em mais peda√ßos (`Chunks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√µes dos chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(rl4rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings de cada documento + busca sem√¢ntica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,\n",
    "              environment=PINECONE_API_ENV)\n",
    "index_name = 'rl4rs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passa todos os embeddings e guarda no index criado na conta em Pinecone\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lan√ßando a query e vendo os resultados mais similares\n",
    "query = 'What has been the economic relevance of recommender systems in industry?'\n",
    "docs = docsearch.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation'),\n",
       " Document(page_content='sequential recommendation methods) is well enough and shows\\nthe shortcomings of benchmarking on datasets collected from tra-\\nditional scenarios. In practice, the data understanding tool may\\nhelp us understand the dataset in the early stages, catch improperly\\ndefined RL problems, and lead to a better problem definition (e.g.,\\nreward definition).\\nTable 3: The comparison of average scores of decode se-\\nquence between different datasets.\\nScore of 5% 20% greedy hot 5% hot 20%\\nRecSys15 1.00 0.53 1.26 0.81 0.42\\nMovieLens 1.00 0.64 0.99 0.98 0.62\\nLast.fm 1.00 0.47 1.09 0.90 0.49\\nCIKMCup2016 1.00 0.30 4.50 0.99 0.28\\nRL4RS-Slate 1.00 0.76 0.62 0.01 0.01\\n5 EVALUATION FRAMEWORK\\nUnlike academic research, in real applications, it is usually rare to\\nhave access to a perfect simulator. A policy should be well evaluated\\nbefore deployment because policy deployment affects the real world\\nand can be costly. In applied settings, policy evaluation is not a'),\n",
       " Document(page_content='must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY\\n¬©2018 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nFigure 1: Several novel recommendation scenarios.\\n1 INTRODUCTION\\nIn 2022, retail e-commerce sales worldwide amounted to 5.7 trillion\\nUS dollars, and e-retail revenues are projected to grow to 6.3 trillion\\nUS dollars in 2023. Such rapid growth promises an excellent future\\nfor the worldwide e-commerce industry, signifying a strong market\\nand increased customer demand. Besides the massive increment\\nof traffic volume, there has been a rapid growth of various rec-\\nommendation scenarios, including slate recommendation (lists of'),\n",
       " Document(page_content='RL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n‚Ä¢Information systems ‚ÜíRecommender systems ;‚Ä¢Comput-\\ning methodologies ‚ÜíReinforcement learning .\\nKEYWORDS')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consultando os documentos para obtermos nossa resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The economic relevance of recommender systems in industry has been significant, with the e-commerce industry projected to grow to 6.3 trillion US dollars in 2023. This growth is driven in part by the rapid increase in various recommendation scenarios, such as slate recommendation and sequential recommendation, which aim to maximize user satisfaction and increase sales. However, there are challenges in implementing these systems, and the RL4RS (Reinforcement Learning for Recommender Systems) resource has been developed to address these challenges and contribute to research in applied reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(input_documents=docs, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chat implementando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Antes de implementar RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='Voc√™ √© um bot assistente gentil e prestativo.'),\n",
    "    HumanMessage(content='Ol√°, como voc√™ est√° hoje?'),\n",
    "    AIMessage(content='Eu estou bem, obrigado por perguntar. Como eu possso te ajudar?'),\n",
    "    HumanMessage(content='Eu gostaria de saber mais sobre sistemas de recomenda√ß√£o baseados em reinforcement learning.')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Claro! Sistemas de recomenda√ß√£o baseados em reinforcement learning (aprendizado por refor√ßo) s√£o uma abordagem que utiliza t√©cnicas de machine learning para recomendar itens ou a√ß√µes personalizadas aos usu√°rios.\\n\\nO aprendizado por refor√ßo √© uma t√©cnica de machine learning onde um agente aprende a tomar decis√µes em um ambiente para maximizar uma recompensa. No caso de sistemas de recomenda√ß√£o, o agente √© o sistema de recomenda√ß√£o em si e o ambiente √© o conjunto de usu√°rios e itens dispon√≠veis para recomenda√ß√£o.\\n\\nNesse tipo de sistema, o sistema de recomenda√ß√£o recebe informa√ß√µes sobre as prefer√™ncias e a√ß√µes dos usu√°rios, bem como o feedback sobre as recomenda√ß√µes anteriores. Com base nessas informa√ß√µes, ele aprende a tomar decis√µes sobre quais itens recomendar para cada usu√°rio da maneira mais efetiva poss√≠vel.\\n\\nO objetivo √© encontrar um equil√≠brio entre explorar novos itens desconhecidos para os usu√°rios e explorar itens j√° conhecidos e mais prov√°veis de serem aceitos. O sistema de recomenda√ß√£o utiliza algoritmos de reinforcement learning para aprender a fazer essas escolhas de forma otimizada, levando em considera√ß√£o as recompensas recebidas pelos usu√°rios.\\n\\nEsses sistemas s√£o especialmente √∫teis em ambientes onde as prefer√™ncias dos usu√°rios podem mudar ao longo do tempo ou onde h√° uma ampla gama de itens dispon√≠veis para recomenda√ß√£o. Eles podem ser aplicados em diferentes contextos, como recomenda√ß√£o de produtos, personaliza√ß√£o de conte√∫do, recomenda√ß√£o de m√∫sicas, entre outros.\\n\\nNo entanto, √© importante notar que sistemas de recomenda√ß√£o baseados em reinforcement learning podem ser complexos de implementar e requerem grande quantidade de dados para aprendizado efetivo. Al√©m disso, a √©tica e a privacidade dos usu√°rios devem ser consideradas ao aplicar essas t√©cnicas.\\n\\nEspero que essa explica√ß√£o tenha sido √∫til! Se voc√™ tiver mais perguntas, estou aqui para ajudar.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Sistemas de recomenda√ß√£o baseados em reinforcement learning (aprendizado por refor√ßo) s√£o uma abordagem que utiliza t√©cnicas de machine learning para recomendar itens ou a√ß√µes personalizadas aos usu√°rios.\n",
      "\n",
      "O aprendizado por refor√ßo √© uma t√©cnica de machine learning onde um agente aprende a tomar decis√µes em um ambiente para maximizar uma recompensa. No caso de sistemas de recomenda√ß√£o, o agente √© o sistema de recomenda√ß√£o em si e o ambiente √© o conjunto de usu√°rios e itens dispon√≠veis para recomenda√ß√£o.\n",
      "\n",
      "Nesse tipo de sistema, o sistema de recomenda√ß√£o recebe informa√ß√µes sobre as prefer√™ncias e a√ß√µes dos usu√°rios, bem como o feedback sobre as recomenda√ß√µes anteriores. Com base nessas informa√ß√µes, ele aprende a tomar decis√µes sobre quais itens recomendar para cada usu√°rio da maneira mais efetiva poss√≠vel.\n",
      "\n",
      "O objetivo √© encontrar um equil√≠brio entre explorar novos itens desconhecidos para os usu√°rios e explorar itens j√° conhecidos e mais prov√°veis de serem aceitos. O sistema de recomenda√ß√£o utiliza algoritmos de reinforcement learning para aprender a fazer essas escolhas de forma otimizada, levando em considera√ß√£o as recompensas recebidas pelos usu√°rios.\n",
      "\n",
      "Esses sistemas s√£o especialmente √∫teis em ambientes onde as prefer√™ncias dos usu√°rios podem mudar ao longo do tempo ou onde h√° uma ampla gama de itens dispon√≠veis para recomenda√ß√£o. Eles podem ser aplicados em diferentes contextos, como recomenda√ß√£o de produtos, personaliza√ß√£o de conte√∫do, recomenda√ß√£o de m√∫sicas, entre outros.\n",
      "\n",
      "No entanto, √© importante notar que sistemas de recomenda√ß√£o baseados em reinforcement learning podem ser complexos de implementar e requerem grande quantidade de dados para aprendizado efetivo. Al√©m disso, a √©tica e a privacidade dos usu√°rios devem ser consideradas ao aplicar essas t√©cnicas.\n",
      "\n",
      "Espero que essa explica√ß√£o tenha sido √∫til! Se voc√™ tiver mais perguntas, estou aqui para ajudar.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pe√ßo desculpas, mas n√£o estou familiarizado com um framework chamado RL4RS. N√£o consigo fornecer informa√ß√µes detalhadas sobre isso. No entanto, fique √† vontade para perguntar sobre outros frameworks ou detalhes espec√≠ficos sobre reinforcement learning ou sistemas de recomenda√ß√£o. Estou aqui para ajudar!\n"
     ]
    }
   ],
   "source": [
    "# Vamos adicionar a √∫ltima resposta do chat √†s mensagens\n",
    "messages.append(res)\n",
    "\n",
    "# criando um novo user prompt\n",
    "prompt = HumanMessage(\n",
    "    content='Voc√™ conhece o framework RL4RS?'\n",
    ")\n",
    "\n",
    "# adicionando √†s mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Inserindo contexto (conceito de RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = docsearch.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta n√£o estiver relacionada ao contexto, esque√ßa o contexto dado e responda como voc√™ normalmente responderia, pois voc√™ ainda √© um assistente prestativo. E se apesar da pergunta n√£o estiver relacionada voc√™ n√£o saber a resposta, apenas diga que n√£o sabe como normalmente voc√™ faria.\n",
    "\n",
    "    Contextos:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, agora, na resposta dada ap√≥s a inser√ß√£o de contexto utilizando o pr√≥prio artigo do framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim, eu conhe√ßo o framework RL4RS. O RL4RS (Reinforcement Learning for Recommender Systems) √© um recurso desenvolvido pela Netease, que consiste em um conjunto de dados de c√≥digo aberto para sistemas de recomenda√ß√£o baseados em reinforcement learning. Ele inclui conjuntos de dados em larga escala, ambientes de simula√ß√£o reprodut√≠veis, baselines avan√ßados de RL relacionados e ferramentas de compreens√£o de dados.\n",
      "\n",
      "O RL4RS tamb√©m oferece um framework de avalia√ß√£o sistem√°tica, que inclui avalia√ß√£o da simula√ß√£o do ambiente, avalia√ß√£o do ambiente de simula√ß√£o e avalia√ß√£o da pol√≠tica contrafactual. Ele foi constru√≠do em Python e utiliza TensorFlow para modelagem e treinamento.\n",
      "\n",
      "O objetivo do RL4RS √© atender √† crescente demanda por sistemas de RL adaptados a cen√°rios de recomenda√ß√£o inovadores. Ele fornece uma plataforma para o desenvolvimento e avalia√ß√£o de sistemas de recomenda√ß√£o baseados em RL.\n",
      "\n",
      "Voc√™ pode encontrar o RL4RS no reposit√≥rio do GitHub da Netease, no seguinte link: https://github.com/fuxiAIlab/RL4RS.\n",
      "\n",
      "Espero que isso esclare√ßa sua d√∫vida! Se voc√™ tiver mais perguntas, estou aqui para ajudar.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt('Voc√™ conhece o framework RL4RS?')\n",
    ")\n",
    "\n",
    "# adicionando √†s mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Estou aqui para ajudar. Por favor, diga-me qual √© sua pergunta ou problema espec√≠fico, e eu farei o poss√≠vel para fornecer orienta√ß√£o ou suporte.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt('Voc√™ poderia me ajudar?')\n",
    ")\n",
    "\n",
    "# adicionando √†s mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# requisitando a resposta do chat\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Gerando chat com contexto inserido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = [SystemMessage(content='Voc√™ √© um assistente gentil e prestativo. Busque responder as perguntas quando conseguir!')]\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=augment_prompt(message)))\n",
    "    gpt_response = chat(history_langchain_format)\n",
    "    return gpt_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(predict).launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Assistente de voz com contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Voc√™ √© um assistente gentil e prestativo e que responde no idioma do usu√°rio.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta n√£o estiver relacionada ao contexto, esque√ßa o contexto dado e responda como voc√™ normalmente responderia, pois voc√™ ainda √© um assistente prestativo. E se apesar da pergunta n√£o estiver relacionada voc√™ n√£o saber a resposta, apenas diga que n√£o sabe como normalmente voc√™ faria.\\n\\n    Contextos:\\n    2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel\\n2https://github.com/google-research/recsim\\n3https://github.com/eyounx/VirtualTaobao\\n4https://github.com/awarebayes/RecNN\\n5https://github.com/google-research/recsim\\n6https://github.com/xinshi-chen/GenerativeAdversarialUserModel\\nSection 3), and are updated weekly. We employ metrics includ-\\ning IPV (Individual Page View), CVR (Conversion Rate), and GMV\\n(Gross Merchandise Volume).\\nDuring the A/B test in one month, the averaged results are pre-\\nsented in Table 9. On each day of the test, the PPO algorithm can\\nlead to more GMV than the DIEN, A2C, and DQN (an increase of\\n23.9%,8.4%,ùëéùëõùëë 12.3%, respectively). Compared with the greedy rec-\\nommendation strategy, which recommends only the items that users\\nare interested in at each step, RL methods can make full use of the\\nmulti-step decision-making characteristics of this scenario, and min-\\nimize the loss of CVR while mixing the recommended items users\\nare interested in and other items. In addition, the performance rank-\\ning of online deployment (PPO>A2C>DQN>SL) is consistent with\\nthe ranking in the simulation environment (PPO>A2C>DQN>SL).\\nIt shows that offline experiments can replace live experiments to\\na certain extent. Here, we promise that we will help researchers\\n\\n    Query: Ol√°, voc√™ poderia me ajudar?\\n'}]\n",
      "[{'role': 'system', 'content': 'Voc√™ √© um assistente gentil e prestativo e que responde no idioma do usu√°rio.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta n√£o estiver relacionada ao contexto, esque√ßa o contexto dado e responda como voc√™ normalmente responderia, pois voc√™ ainda √© um assistente prestativo. E se apesar da pergunta n√£o estiver relacionada voc√™ n√£o saber a resposta, apenas diga que n√£o sabe como normalmente voc√™ faria.\\n\\n    Contextos:\\n    ommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation\\nommendation scenarios, including slate recommendation (lists of\\nitems), bundle recommendation (a collection of items that should be\\npurchased simultaneously), sequential recommendation, and many\\nothers, as shown in Figure 1. It is worth exploring the various chal-\\nlenges the modern e-commerce industry faces today. Most current\\ne-commerce and retail companies build their recommender systems\\nby implementing supervised learning based algorithms on their\\nwebsites to maximize immediate user satisfaction in a greedy man-\\nner. However, the item-wise greedy recommendation strategy is an\\nimperfect fitting to real recommendation systems. With more and\\nmore new upcoming recommendation scenarios, more and more\\nchallenges have to be solved. For instance, in sequential recom-\\nmendation scenarios, traditional methods often consider different\\nrecommendation steps in a session to be independent and fail to\\nmaximize the expected accumulative utilities in a recommendation\\nRL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n‚Ä¢Information systems ‚ÜíRecommender systems ;‚Ä¢Comput-\\ning methodologies ‚ÜíReinforcement learning .\\nKEYWORDS\\n\\n    Query: Me d√™ uma breve explica√ß√£o sobre sistemas de recomenda√ß√£o.\\n'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\applications.py\", line 116, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 746, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\responses.py\", line 340, in __call__\n",
      "    await send(\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 512, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\asyncio\\proactor_events.py\", line 162, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] Foi for√ßado o cancelamento de uma conex√£o existente pelo host remoto\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\applications.py\", line 116, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 746, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 55, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 44, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\responses.py\", line 340, in __call__\n",
      "    await send(\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\_exception_handler.py\", line 41, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 512, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Voc√™ √© um assistente gentil e prestativo e que responde no idioma do usu√°rio.'}, {'role': 'user', 'content': 'Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta n√£o estiver relacionada ao contexto, esque√ßa o contexto dado e responda como voc√™ normalmente responderia, pois voc√™ ainda √© um assistente prestativo. E se apesar da pergunta n√£o estiver relacionada voc√™ n√£o saber a resposta, apenas diga que n√£o sabe como normalmente voc√™ faria.\\n\\n    Contextos:\\n    RL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n‚Ä¢Information systems ‚ÜíRecommender systems ;‚Ä¢Comput-\\ning methodologies ‚ÜíReinforcement learning .\\nKEYWORDS\\nRL-based RS suffers from the difficulties of being well-validated\\nbefore deployment. We attempt to propose a new systematic evalu-\\nation framework, including evaluation of environment simulation,\\nevaluation on environments, counterfactual policy evaluation, and\\nevaluation on environments built from test set. In summary, the\\nRL4RS (Reinforcement Learning for Recommender Systems), a new\\nresource with special concerns on the reality gaps, contains two\\nreal-world datasets, data understanding tools, tuned simulation\\nenvironments, related advanced RL baselines, batch RL baselines,\\nand counterfactual policy evaluation algorithms. The RL4RS suite\\ncan be found at https://github.com/fuxiAIlab/RL4RS . In addition\\nto the RL-based recommender systems, we expect the resource to\\ncontribute to research in applied reinforcement learning.\\nCCS CONCEPTS\\n‚Ä¢Information systems ‚ÜíRecommender systems ;‚Ä¢Comput-\\ning methodologies ‚ÜíReinforcement learning .\\nKEYWORDS\\nRL4RS: A Real-World Dataset for Reinforcement\\nLearning based Recommender System\\nKai Wang1, Zhene Zou1, Minghao Zhao1, Qilin Deng1, Yue Shang1, Yile Liang1,\\nRunze Wu1, Xudong Shen1, Tangjie Lyu1, Changjie Fan1\\n1Fuxi AI Lab, NetEase Inc., Hangzhou, China\\n{wangkai02,zouzhene, zhaominghao,dengqilin,shangyue,liangyile,\\nwurunze1,hzshenxudong ,hzlvtangjie,fanchangjie}\\n@corp.netease.com\\nABSTRACT\\nReinforcement learning based recommender systems (RL-based\\nRS) aim at learning a good policy from a batch of collected data,\\nby casting recommendations to multi-step decision-making tasks.\\nHowever, current RL-based RS research commonly has a large real-\\nity gap. In this paper, we introduce the first open-source real-world\\ndataset, RL4RS, hoping to replace the artificial datasets and semi-\\nsimulated RS datasets previous studies used due to the resource\\nlimitation of the RL-based RS domain. Unlike academic RL research,\\nRL-based RS suffers from the difficulties of being well-validated\\n\\n    Query: Voc√™ conhece o framework RL4RS, que √© a Real World Dataset for Reforcement Learning Based Recommender System?\\n'}]\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def transcribe2(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    sf.write(\"output_audio.wav\", y, sr, format='WAV', subtype='PCM_16')\n",
    "\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=open('output_audio.wav', 'rb'),\n",
    "        response_format='text'\n",
    "    )\n",
    "    return transcript\n",
    "\n",
    "def voice_chat(user_voice):\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente gentil e prestativo e que responde no idioma do usu√°rio.\"},\n",
    "    ]       \n",
    "    \n",
    "    user_message = transcribe2(user_voice)\n",
    "\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": augment_prompt(user_message)},\n",
    "    )\n",
    "\n",
    "    print(messages)\n",
    "\n",
    "    chat = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=messages\n",
    "    )\n",
    "    \n",
    "    reply = chat.choices[0].message.content\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=reply\n",
    "    )\n",
    "    response.stream_to_file('output.mp3')\n",
    "\n",
    "    return (reply, 'output.mp3')\n",
    "\n",
    "\n",
    "text_reply = gr.Textbox(label=\"ChatGPT Text\")\n",
    "voice_reply = gr.Audio('output.mp3')\n",
    "\n",
    "demo = gr.Interface(\n",
    "    voice_chat,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    outputs=[text_reply, voice_reply],\n",
    "    title = 'AI Voice Assistant with ChatGPT AI'\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
