{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perguntas sobre RecSys-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lendo a chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98a62383-2f1c-4112-91a4-4ced24915603'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando o paper sobre a implementação de RL em RS com feedback de usuário explícito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutton - RL\n",
    "loader_paper = PyPDFLoader('../PDF/recsys-rl.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_paper = loader_paper.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Fig. 1. Analysis on sequential patterns on user’s behavior in MovieLens and\\nYahoo!Music datasets\\nclick and read the two pieces of news with equal probability,\\nwhere one is about a thunderstorm alert and the other is about\\na basketball player Kobe Bryant. In this example, after reading\\nthe news about thunderstorm, the user probably is not willing\\nto read news about this issue anymore; while on the other hand,\\nthe user will possibly read more about NBA or basketball\\nafter reading the news about Kobe. The fact suggests that\\nrecommending the news about Kobe will introduce more long-\\nterm rewards. Hence, when recommending items to users, both\\nthe immediate and long-term rewards should be taken into\\nconsideration.\\nRecently, Reinforcement Learning (RL) [20], which has\\nshown great potential in various challenging scenarios that\\nrequire both dynamic modeling and long term planning, such\\nas game playing [21], [22], real-time ads bidding [23], [24],\\nneural network structure searching [25], [26], is introduced in\\nrecommender systems [18], [19], [27]–[33].\\nIn the early stage, model-based RL techniques are proposed\\nto model recommendation procedure, such as POMDP [18]\\nand Q-learning [27]. However, these methods are inapplicable\\nto complicated recommendation scenarios when the number\\nof candidate items is large, because a time-consuming dy-\\nnamic programming step is required to update the model.\\nLater, model-free RL techniques are utilized in recommender\\nsystems, from both academia and industry. Such techniques\\ncan be divided into two categories: value-based [19], [29] and\\npolicy-based [28], [32], [33]. Value-based approaches compute\\nQ-values of all available actions for a given state and the\\none with the maximum Q-value is selected as the best action.\\nDue to the evaluation on overall actions, the approaches may\\nbecome very inefﬁcient if the action space is too large. As\\nfor the policy-based approaches, this type of studies generate\\na continuous parameter vector as the representation of an\\naction [28], [32], [33], which can be utilized in generating the\\nrecommendation and updating the Q-value evaluator. Thanks\\nto the continuous representations, the inefﬁciency drawbacks\\ncan be overcome. However, these studies [28], [32], [33] still\\nhave one common limitation: the user state is learnt via a\\nconventional fully connected neural network, which does not\\nexplicitly and carefully model the interactions between users\\nand items.\\nIn this paper, to break the limitations stated above, we\\npropose a d eep r einforcement learning based r ecommendation\\nframework with explicit user-item interactions modeling(DRR). The “Actor-Critic” type framework DRR is incor-\\nporated with a state representation module, which explicitly\\nmodels the complex dynamic user-item interactions to pursuit\\nbetter recommendation performance. Speciﬁcally, the embed-\\ndings of users and items from the historical interactions are fed\\ninto a carefully designed multi-layer network, which explicitly\\nmodels the interactions between users and items, to produce\\na continuous state representation of the user in terms of her\\nunderlying sequential behaviors. This network is named as\\nthe state representation module, which plays two important\\nroles in our framework. On the one hand, it is utilized to\\ngenerate an ranking action to calculate the recommendation\\nscores for ranking. On the other hand, the state representation\\ntogether with the generated action is the input of the Critic\\nnetwork, which aims to estimate the Q-value, i.e., the quality\\nof the action in the current state. Based on the evaluation, the\\nActor (policy) network can be updated. We note that both the\\nActor and Critic networks are carefully designed by modeling\\nthe interactions between users and items explicitly. Extensive\\nexperiments on four real-world datasets demonstrate that the\\nproposed method yields superior performance than the state-\\nof-the-art methods. The main contributions of this paper can\\nbe summarized as follows:\\n•We propose a deep reinforcement learning based rec-\\nommendation framework DRR. Unlike the conventional\\nstudies, DRR adopts an “Actor-Critic” structure and treats\\nthe recommendation as a sequential decision making\\nprocess, which takes both the immediate and long-term\\nrewards into consideration.\\n•Under the DRR framework, three different network struc-\\ntures are proposed, which can explicitly model the inter-\\nactions between users and items.\\n•Extensive experiments are carried out on four real-world\\ndatasets, and the results demonstrate the proposed meth-\\nods indeed outperforms the state-of-the-art competitors.\\nThe rest of this paper is organized as follows. Related work\\nand background are presented in Section II. The preliminary\\nknowledge is presented in Section III. The proposed methods\\nare introduced in Section IV . Experimental details and results\\nare discussed in Section V . Finally, we conclude this paper\\nand discuss some future work in Section VI.\\nII. R ELATED WORK\\nA. Non-RL based Recommendation Techniques\\nVarious kinds of recommendation techniques are proposed\\nin the past a few decades to improve the performance of\\nrecommender systems, including content-based ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], and until\\nrecently deep learning models [9]–[12].\\nAt the beginning of this century, content-based ﬁltering [1]\\nis proposed to recommend items by considering the content\\nsimilarity between items. Later, collaborative ﬁltering (CF) is\\nput forward and extensively studied. The rationale behind CF\\nis that the users with similar behaviors tend to prefer the same', metadata={'source': '../PDF/recsys-rl.pdf', 'page': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recsys_paper[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import NotebookLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_jupyter = NotebookLoader(\n",
    "    \"../Others/recsys_rl.ipynb\",\n",
    "    include_outputs=True,\n",
    "    max_output_length=200,\n",
    "    remove_newline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\langchain_community\\document_loaders\\notebook.py:122: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n"
     ]
    }
   ],
   "source": [
    "recsys_jupyter = loader_jupyter.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\'code\\' cell: \\'[\\'from collections import defaultdict\\', \\'import os\\', \\'import pickle\\', \\'import random\\', \\'import requests\\', \\'import time\\', \\'import tqdm\\', \\'\\', \\'from IPython.core.debugger import set_trace\\', \\'import numpy as np\\', \\'import pandas as pd\\', \\'from pytorch_ranger import Ranger\\', \\'import torch\\', \\'import torch.nn as nn\\', \\'import torch.nn.functional as F \\', \\'import torch.utils.data as td\\', \\'from torch.utils.tensorboard import SummaryWriter\\', \\'\\', \\'from utils import (EvalDataset, OUNoise, Prioritized_Buffer, get_beta, \\', \\'                   preprocess_data, to_np, hit_metric, dcg_metric)\\']\\'\\n\\n \\'code\\' cell: \\'[\\'data_dir = \"data\"\\', \\'rating = \"ml-1m.train.rating\"\\', \\'\\', \\'params = {\\', \"    \\'batch_size\\': 512,\", \"    \\'embedding_dim\\': 8,\", \"    \\'hidden_dim\\': 16,\", \"    \\'N\\': 5, # memory size for state_repr\", \"    \\'ou_noise\\':False,\", \\'    \\', \"    \\'value_lr\\': 1e-5,\", \"    \\'value_decay\\': 1e-4,\", \"    \\'policy_lr\\': 1e-5,\", \"    \\'policy_decay\\': 1e-6,\", \"    \\'state_repr_lr\\': 1e-5,\", \"    \\'state_repr_decay\\': 1e-3,\", \"    \\'log_dir\\': \\'logs/final/\\',\", \"    \\'gamma\\': 0.8,\", \"    \\'min_value\\': -10,\", \"    \\'max_value\\': 10,\", \"    \\'soft_tau\\': 1e-3,\", \\'    \\', \"    \\'buffer_size\\': 1000000\", \\'}\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 0. Problem statement\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Traditional recommendation task can be treated as sequental desicion making problem.\\', \\'Recommender (i.e. agent) interacts with users (i.e. environment) to sequentally suggest set of items.\\', \"The goal is to maximize clients\\' satisfaction (i.e. reward).\", \\'More specifically:\\', \\'- State is a vector $a \\\\\\\\in R^{3\\\\\\\\cdot embedding\\\\\\\\_dim}$ computed using the user embedding and the embeddings of `N` latest positive interactions. In the code (replay buffer) state is represented  by `(user, memory)`\\', \\'- Action is a vector $a \\\\\\\\in R^{embedding\\\\\\\\_dim}$. To get ranking score we took dot product of\\', \\'the action and the item embedding (similar to word2vec and other embedding models).\\', \\'- Reward is taken from user-item matrix (1 if rating > 3, 0 otherwise)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Reinforcement Learning can help recommendation at least in 2 ways.\\', \\'1. User’s preference on previous items will affect his choice on the next items. \\', \\'User tends to give a higher rating if he has consecutively received more satisfied items (and vice versa). \\', \\'So, it would be more reasonable to model the recommendation as a sequential decision making process.\\', \\'2. It is important to use long-term planning in recommendations. For example, after reading the weather forecast, the user is not willing\\', \\'to read similar news. On the other hand, after watching funny videos or reading memes the user can constanly do the same.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'# Movielens (1M) data from the https://github.com/hexiangnan/neural_collaborative_filtering\\', \"if not os.path.isdir(\\'./data\\'):\", \"    os.mkdir(\\'./data\\')\", \\'    \\', \\'file_path = os.path.join(data_dir, rating)\\', \\'if os.path.exists(file_path):\\', \\'    print(\"Skip loading \" + file_path)\\', \\'else:\\', \\'    with open(file_path, \"wb\") as tf:\\', \\'        print(\"Load \" + file_path)\\', \\'        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + rating)\\', \\'        tf.write(r.content)\\', \\'        \\', \\'(train_data, train_matrix, test_data, test_matrix, \\', \\' user_num, item_num, appropriate_users) = preprocess_data(data_dir, rating)\\']\\'\\n with output: \\'[\\'Skip loading data/ml-1m.train.rating\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 1. Environment\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'- **Observation space**. As mentioned before, to get state we need `N` latest positive items (`memory`) and embedding of user. `State_Repr_Module` transform it to the vector of dimensionality `embedding_dim * 3`.\\', \\'\\', \"- **Action space**. For every user we sample nonrelated items (the same count as related). All `available_items` which wasn\\'t viewed before form action space.\", \\'\\', \\'Given a state we get action embedding, compute dot product between this embedding and embeddings of all items in action space, take 1 top ranked item, compute reward, update `viewed_items` and memory, and store transition in buffer.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'class Env():\\', \\'    def __init__(self, user_item_matrix):\\', \\'        self.matrix = user_item_matrix\\', \\'        self.item_count = item_num\\', \"        self.memory = np.ones([user_num, params[\\'N\\']]) * item_num\", \\'        # memory is initialized as [item_num] * N for each user\\', \\'        # it is padding indexes in state_repr and will result in zero embeddings\\', \\'\\', \\'    def reset(self, user_id):\\', \\'        self.user_id = user_id\\', \\'        self.viewed_items = []\\', \\'        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\\', \\'        self.num_rele = len(self.related_items)\\', \\'        self.nonrelated_items = np.random.choice(\\', \\'            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\\', \\'        self.available_items = np.zeros(self.num_rele * 2)\\', \\'        self.available_items[::2] = self.related_items\\', \\'        self.available_items[1::2] = self.nonrelated_items\\', \\'        \\', \\'        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :])\\', \\'    \\', \\'    def step(self, action, action_emb=None, buffer=None):\\', \\'        initial_user = self.user_id\\', \\'        initial_memory = self.memory[[initial_user], :]\\', \\'        \\', \\'        reward = float(to_np(action)[0] in self.related_items)\\', \\'        self.viewed_items.append(to_np(action)[0])\\', \\'        if reward:\\', \\'            if len(action) == 1:\\', \\'                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\\', \\'            else:\\', \\'                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action[0]]\\', \\'                \\', \\'        if len(self.viewed_items) == len(self.related_items):\\', \\'            done = 1\\', \\'        else:\\', \\'            done = 0\\', \\'            \\', \\'        if buffer is not None:\\', \\'            buffer.push(np.array([initial_user]), np.array(initial_memory), to_np(action_emb)[0], \\', \\'                        np.array([reward]), np.array([self.user_id]), self.memory[[self.user_id], :], np.array([reward]))\\', \\'\\', \\'        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :]), reward, done\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 2. Model\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Overall model\\', \\'\\', \\'<img src=\"img/full_model.png\" width=\"500\" height=\"350\">\\']\\'\\n\\n \\'code\\' cell: \\'[\\'class Actor_DRR(nn.Module):\\', \\'    def __init__(self, embedding_dim, hidden_dim):\\', \\'        super().__init__()\\', \\'    \\', \\'        self.layers = nn.Sequential(\\', \\'            nn.Linear(embedding_dim * 3, hidden_dim),\\', \\'            nn.ReLU(),\\', \\'            nn.Linear(hidden_dim, embedding_dim)\\', \\'        )\\', \\'        \\', \\'        self.initialize()\\', \\'\\', \\'    def initialize(self):\\', \\'        for layer in self.layers:\\', \\'            if isinstance(layer, nn.Linear):\\', \\'                nn.init.kaiming_uniform_(layer.weight)\\', \\'\\', \\'    def forward(self, state):\\', \\'        return self.layers(state)\\', \\'    \\', \\'    def get_action(self, user, memory, state_repr, \\', \\'                   action_emb,\\', \\'                   items=torch.tensor([i for i in range(item_num)]),\\', \\'                   return_scores=False\\', \\'                  ):\\', \\'        state = state_repr(user, memory)\\', \\'        scores = torch.bmm(state_repr.item_embeddings(items).unsqueeze(0), \\', \\'                         action_emb.T.unsqueeze(0)).squeeze(0)\\', \\'        if return_scores:\\', \\'            return scores, torch.gather(items, 0, scores.argmax(0))\\', \\'        else:\\', \\'            return torch.gather(items, 0, scores.argmax(0))\\']\\'\\n\\n \\'code\\' cell: \\'[\\'class Critic_DRR(nn.Module):\\', \\'    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\\', \\'        super().__init__()\\', \\'\\', \\'        self.layers = nn.Sequential(\\', \\'            nn.Linear(state_repr_dim + action_emb_dim, hidden_dim), \\', \\'            nn.ReLU(), \\', \\'            nn.Linear(hidden_dim, 1)\\', \\'        )\\', \\'\\', \\'        self.initialize()\\', \\'        \\', \\'    def initialize(self):\\', \\'        for layer in self.layers:\\', \\'            if isinstance(layer, nn.Linear):\\', \\'                nn.init.kaiming_uniform_(layer.weight)\\', \\'        \\', \\'    def forward(self, state, action):\\', \\'        x = torch.cat([state, action], 1)\\', \\'        x = self.layers(x)\\', \\'        return x\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### State representation\\', \\'\\', \\'<img src=\"img/state_representation.png\" width=\"350\" height=\"250\">\\']\\'\\n\\n \\'code\\' cell: \\'[\\'class State_Repr_Module(nn.Module):\\', \\'    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\\', \\'        super().__init__()\\', \\'        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\\', \\'        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\\', \"        self.drr_ave = torch.nn.Conv1d(in_channels=params[\\'N\\'], out_channels=1, kernel_size=1)\", \\'        \\', \\'        self.initialize()\\', \\'            \\', \\'    def initialize(self):\\', \\'        nn.init.normal_(self.user_embeddings.weight, std=0.01)\\', \\'        nn.init.normal_(self.item_embeddings.weight, std=0.01)\\', \\'        self.item_embeddings.weight.data[-1].zero_()\\', \\'        nn.init.uniform_(self.drr_ave.weight)\\', \\'        self.drr_ave.bias.data.zero_()\\', \\'\\', \\'    def forward(self, user, memory):\\', \\'        user_embedding = self.user_embeddings(user.long())\\', \\'\\', \\'        item_embeddings = self.item_embeddings(memory.long())\\', \\'        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\\', \\'        \\', \\'        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'For evaluation we take 1 positive and 99 sampled negatives items per batch, select 10 items with best scores and calculate hit_rate@10 and nDCG@10.\\', \\'During training we choose user 6039 and track `hit` and `dcg` only for him (for evaluation speed). Final scores was computed on the whole test data.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'valid_dataset = EvalDataset(\\', \\'    np.array(test_data)[np.array(test_data)[:, 0] == 6039], \\', \\'    item_num, \\', \\'    test_matrix)\\', \\'valid_loader = td.DataLoader(valid_dataset, batch_size=100, shuffle=False)\\', \\'\\', \\'full_dataset = EvalDataset(np.array(test_data), item_num, test_matrix)\\', \\'full_loader = td.DataLoader(full_dataset, batch_size=100, shuffle=False)\\']\\'\\n with output: \\'[\\'Resetting dataset\\\\n\\', \\'Resetting dataset\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\\'def run_evaluation(net, state_representation, training_env_memory, loader=valid_loader):\\', \\'    hits = []\\', \\'    dcgs = []\\', \\'    test_env = Env(test_matrix)\\', \\'    test_env.memory = training_env_memory.copy()\\', \"    user, memory = test_env.reset(int(to_np(next(iter(valid_loader))[\\'user\\'])[0]))\", \\'    for batch in loader:\\', \\'        action_emb = net(state_repr(user, memory))\\', \\'        scores, action = net.get_action(\\', \"            batch[\\'user\\'], \", \"            torch.tensor(test_env.memory[to_np(batch[\\'user\\']).astype(int), :]), \", \\'            state_representation, \\', \\'            action_emb,\\', \"            batch[\\'item\\'].long(), \", \\'            return_scores=True\\', \\'        )\\', \\'        user, memory, reward, done = test_env.step(action)\\', \\'\\', \\'        _, ind = scores[:, 0].topk(10)\\', \"        predictions = torch.take(batch[\\'item\\'], ind).cpu().numpy().tolist()\", \"        actual = batch[\\'item\\'][0].item()\", \\'        hits.append(hit_metric(predictions, actual))\\', \\'        dcgs.append(dcg_metric(predictions, actual))\\', \\'        \\', \\'    return np.mean(hits), np.mean(dcgs)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 3. Training\\']\\'\\n\\n \\'code\\' cell: \\'[\\'torch.manual_seed(2)\\', \\'\\', \"state_repr = State_Repr_Module(user_num, item_num, params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"policy_net = Actor_DRR(params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"value_net  = Critic_DRR(params[\\'embedding_dim\\'] * 3, params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"replay_buffer = Prioritized_Buffer(params[\\'buffer_size\\'])\", \\'\\', \"target_value_net  = Critic_DRR(params[\\'embedding_dim\\'] * 3, params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"target_policy_net = Actor_DRR(params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \\'\\', \\'for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\\', \\'    target_param.data.copy_(param.data)\\', \\'\\', \\'for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\\', \\'    target_param.data.copy_(param.data)\\', \\'\\', \\'value_criterion  = nn.MSELoss()\\', \"value_optimizer  = Ranger(value_net.parameters(),  lr=params[\\'value_lr\\'], \", \"                          weight_decay=params[\\'value_decay\\'])\", \"policy_optimizer = Ranger(policy_net.parameters(), lr=params[\\'policy_lr\\'], \", \"                          weight_decay=params[\\'policy_decay\\'])\", \"state_repr_optimizer = Ranger(state_repr.parameters(), lr=params[\\'state_repr_lr\\'], \", \"                              weight_decay=params[\\'state_repr_decay\\'])\", \\'\\', \"writer = SummaryWriter(log_dir=params[\\'log_dir\\'])\"]\\'\\n with output: \\'[\\'Ranger optimizer loaded. \\\\n\\', \\'Gradient Centralization usage = True\\\\n\\', \\'GC applied to both conv and fc layers\\\\n\\', \\'Ranger optimizer loaded. \\\\n\\', \\'Gradient Centralization usage = True\\\\n\\', \\'GC applied to both conv and fc layers\\\\n\\', \\'Ranger optimizer loaded. \\\\n\\', \\'Gradient Centralization usage = True\\\\n\\', \\'GC applied to both conv and fc layers\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\\'def ddpg_update(training_env, \\', \\'                step=0,\\', \"                batch_size=params[\\'batch_size\\'], \", \"                gamma=params[\\'gamma\\'],\", \"                min_value=params[\\'min_value\\'],\", \"                max_value=params[\\'max_value\\'],\", \"                soft_tau=params[\\'soft_tau\\'],\", \\'               ):\\', \\'    beta = get_beta(step)\\', \\'    user, memory, action, reward, next_user, next_memory, done = replay_buffer.sample(batch_size, beta)\\', \\'    user        = torch.FloatTensor(user)\\', \\'    memory      = torch.FloatTensor(memory)\\', \\'    action      = torch.FloatTensor(action)\\', \\'    reward      = torch.FloatTensor(reward)\\', \\'    next_user   = torch.FloatTensor(next_user)\\', \\'    next_memory = torch.FloatTensor(next_memory)\\', \\'    done = torch.FloatTensor(done)\\', \\'    \\', \\'    state       = state_repr(user, memory)\\', \\'    policy_loss = value_net(state, policy_net(state))\\', \\'    policy_loss = -policy_loss.mean()\\', \\'    \\', \\'    next_state     = state_repr(next_user, next_memory)\\', \\'    next_action    = target_policy_net(next_state)\\', \\'    target_value   = target_value_net(next_state, next_action.detach())\\', \\'    expected_value = reward + (1.0 - done) * gamma * target_value\\', \\'    expected_value = torch.clamp(expected_value, min_value, max_value)\\', \\'\\', \\'    value = value_net(state, action)\\', \\'    value_loss = value_criterion(value, expected_value.detach())\\', \\'    \\', \\'    state_repr_optimizer.zero_grad()\\', \\'    policy_optimizer.zero_grad()\\', \\'    policy_loss.backward(retain_graph=True)\\', \\'    policy_optimizer.step()\\', \\'\\', \\'    value_optimizer.zero_grad()\\', \\'    value_loss.backward(retain_graph=True)\\', \\'    value_optimizer.step()\\', \\'    state_repr_optimizer.step()\\', \\'\\', \\'    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\\', \\'                target_param.data.copy_(\\', \\'                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\\', \\'                )\\', \\'\\', \\'    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\\', \\'            target_param.data.copy_(\\', \\'                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\\', \\'            )\\', \\'\\', \"    writer.add_histogram(\\'value\\', value, step)\", \"    writer.add_histogram(\\'target_value\\', target_value, step)\", \"    writer.add_histogram(\\'expected_value\\', expected_value, step)\", \"    writer.add_histogram(\\'policy_loss\\', policy_loss, step)\"]\\'\\n\\n \\'code\\' cell: \\'[\\'np.random.seed(16)\\', \\'train_env = Env(train_matrix)\\', \\'hits, dcgs = [], []\\', \\'hits_all, dcgs_all = [], []\\', \\'step, best_step = 0, 0\\', \\'step, best_step, best_step_all = 0, 0, 0\\', \\'users = np.random.permutation(appropriate_users)\\', \"ou_noise = OUNoise(params[\\'embedding_dim\\'], decay_period=10)\", \\'\\', \\'for u in tqdm.tqdm(users):\\', \\'    user, memory = train_env.reset(u)\\', \"    if params[\\'ou_noise\\']:\", \\'        ou_noise.reset()\\', \\'    for t in range(int(train_matrix[u].sum())):\\', \\'        action_emb = policy_net(state_repr(user, memory))\\', \"        if params[\\'ou_noise\\']:\", \\'            action_emb = ou_noise.get_action(action_emb.detach().cpu().numpy()[0], t)\\', \\'        action = policy_net.get_action(\\', \\'            user, \\', \\'            torch.tensor(train_env.memory[to_np(user).astype(int), :]), \\', \\'            state_repr, \\', \\'            action_emb,\\', \\'            torch.tensor(\\', \\'                [item for item in train_env.available_items \\', \\'                if item not in train_env.viewed_items]\\', \\'            ).long()\\', \\'        )\\', \\'        user, memory, reward, done = train_env.step(\\', \\'            action, \\', \\'            action_emb,\\', \\'            buffer=replay_buffer\\', \\'        )\\', \\'\\', \"        if len(replay_buffer) > params[\\'batch_size\\']:\", \\'            ddpg_update(train_env, step=step)\\', \\'\\', \\'        if step % 100 == 0 and step > 0:\\', \\'            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory)\\', \"            writer.add_scalar(\\'hit\\', hit, step)\", \"            writer.add_scalar(\\'dcg\\', dcg, step)\", \\'            hits.append(hit)\\', \\'            dcgs.append(dcg)\\', \\'            if np.mean(np.array([hit, dcg]) - np.array([hits[best_step], dcgs[best_step]])) > 0:\\', \\'                best_step = step // 100\\', \"                torch.save(policy_net.state_dict(), params[\\'log_dir\\'] + \\'policy_net.pth\\')\", \"                torch.save(value_net.state_dict(), params[\\'log_dir\\'] + \\'value_net.pth\\')\", \"                torch.save(state_repr.state_dict(), params[\\'log_dir\\'] + \\'state_repr.pth\\')\", \\'        if step % 10000 == 0 and step > 0:\\', \\'            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory, full_loader)\\', \"            writer.add_scalar(\\'hit_all\\', hit, step)\", \"            writer.add_scalar(\\'dcg_all\\', dcg, step)\", \\'            hits_all.append(hit)\\', \\'            dcgs_all.append(dcg)\\', \\'            if np.mean(np.array([hit, dcg]) - np.array([hits_all[best_step_all], dcgs_all[best_step_all]])) > 0:\\', \\'                best_step_all = step // 10000\\', \"                torch.save(policy_net.state_dict(), params[\\'log_dir\\'] + \\'best_policy_net.pth\\')\", \"                torch.save(value_net.state_dict(), params[\\'log_dir\\'] + \\'best_value_net.pth\\')\", \"                torch.save(state_repr.state_dict(), params[\\'log_dir\\'] + \\'best_state_repr.pth\\')\", \\'        step += 1\\']\\'\\n with output: \\'[\\'100%|██████████| 4699/4699 [5:34:55<00:00,  4.28s/it]   \\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\"torch.save(policy_net.state_dict(), params[\\'log_dir\\'] + \\'policy_net_final.pth\\')\", \"torch.save(value_net.state_dict(), params[\\'log_dir\\'] + \\'value_net_final.pth\\')\", \"torch.save(state_repr.state_dict(), params[\\'log_dir\\'] + \\'state_repr_final.pth\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\"# we need memory for validation, so it\\'s better to save it and not wait next time \", \"with open(\\'logs/memory.pickle\\', \\'wb\\') as f:\", \\'    pickle.dump(train_env.memory, f)\\', \\'    \\', \"with open(\\'logs/memory.pickle\\', \\'rb\\') as f:\", \\'    memory = pickle.load(f)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 4. Results\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Weights and logs are stored in [this folder](https://drive.google.com/drive/folders/1hsGjh8oHN4uyCmp_wtAyVPTR76ylVVgH?usp=sharing)\\']\\'\\n\\n \\'code\\' cell: \\'[\"no_ou_state_repr = State_Repr_Module(user_num, item_num, params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"no_ou_policy_net = Actor_DRR(params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"no_ou_state_repr.load_state_dict(torch.load(\\'logs/no_ou/\\' + \\'best_state_repr.pth\\'))\", \"no_ou_policy_net.load_state_dict(torch.load(\\'logs/no_ou/\\' + \\'best_policy_net.pth\\'))\", \\'    \\', \\'hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, memory, full_loader)\\', \"print(\\'hit rate: \\', hit, \\'dcg: \\', dcg)\"]\\'\\n with output: \\'[\\'hit rate:  0.45371301632835115 dcg:  0.25425667195713686\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\"ou_state_repr = State_Repr_Module(user_num, item_num, params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"ou_policy_net = Actor_DRR(params[\\'embedding_dim\\'], params[\\'hidden_dim\\'])\", \"ou_state_repr.load_state_dict(torch.load(\\'logs/ou_noise_04/\\' + \\'best_state_repr.pth\\'))\", \"ou_policy_net.load_state_dict(torch.load(\\'logs/ou_noise_04/\\' + \\'best_policy_net.pth\\'))\", \\'\\', \\'hit, dcg = run_evaluation(ou_policy_net, ou_state_repr, memory, full_loader)\\', \"print(\\'hit rate: \\', hit, \\'dcg: \\', dcg)\"]\\'\\n with output: \\'[\\'hit rate:  0.5024706798086426 dcg:  0.28013841397710176\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Example of trained agents behaviour\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"Let\\'s choose random user\"]\\'\\n\\n \\'code\\' cell: \\'[\\'random_user = np.random.randint(user_num)\\', \\'print(random_user)\\']\\'\\n with output: \\'[\\'2169\\\\n\\']\\'\\n\\n  \\'markdown\\' cell: \\'[\\'For example we can recommend \"Nixon\" and \"Love Serenade\" and see next 3 predictions.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'predictions = []\\', \\'\\', \\'for model, state_representation in zip([ou_policy_net, no_ou_policy_net], [ou_state_repr, no_ou_state_repr]):\\', \\'    example_env = Env(test_matrix)\\', \\'    user, memory = example_env.reset(random_user)\\', \\'\\', \\'    user, memory, reward, _ = example_env.step(torch.tensor([13]))\\', \\'    user, memory, reward, _ = example_env.step(torch.tensor([1584]))\\', \\'    preds = []\\', \\'    for _ in range(3):\\', \\'        action_emb = model(state_representation(user, memory))\\', \\'        action = model.get_action(\\', \\'            user, \\', \\'            torch.tensor(example_env.memory[to_np(user).astype(int), :]), \\', \\'            state_representation, \\', \\'            action_emb,\\', \\'            torch.tensor(\\', \\'                [item for item in example_env.available_items \\', \\'                if item not in example_env.viewed_items]\\', \\'            ).long()\\', \\'        )\\', \\'        user, memory, reward, _ = example_env.step(action)\\', \\'        preds.append(action)\\', \\'\\', \\'    predictions.append(preds)\\', \\'\\', \\'print(predictions[0])\\', \\'print(predictions[1])\\']\\'\\n with output: \\'[\\'[tensor([124]), tensor([127]), tensor([1228])]\\\\n\\', \\'[tensor([71]), tensor([1967]), tensor([1405])]\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Model trained with OU noise recommended related `Comedy` and `Documentary` after that switch recommendations to nonrelated `Crime|Film-Noir|Thriller`.\\', \"Model trained without OU noise recommended `Comedy|Drama`, `Children\\'s|Comedy`, `Drama` (two of them are related).\", \\'\\', \\'Both models seems to be reasonable.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Training process logs\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'<img src=img/learning_curve.png>\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Logs are consistent with expectations. Adding noise increase metrics (std=0.4 performs the best, after 0.6 model starts to degrade).\\']\\'\\n\\n', metadata={'source': '..\\\\Others\\\\recsys_rl.ipynb'})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recsys_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Juntando os documentos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys = recsys_paper + recsys_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quebrando em mais pedaços (`Chunks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definições dos chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 6000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(recsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings de cada documento + busca semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\cenv\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,\n",
    "              environment=PINECONE_API_ENV)\n",
    "index_name = 'rl4rs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passa todos os embeddings e guarda no index criado na conta em Pinecone\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lançando a query e vendo os resultados mais similares\n",
    "query = 'Quais foram as métricas utilizadas para validação do framework?'\n",
    "docs = docsearch.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='C. Evaluation\\nIn this subsection, we discuss how to evaluate the models\\nwith a environment simulator. The most straightforward way to\\nevaluate the RL based models is to conduct online experiments\\non recommender systems where the recommender directly\\ninteracts with users. However, the underlying commercial risk\\nand the costly deployment on the platform make it impracti-\\ncal. Therefore, throughout the testing phase, we conduct the\\nevaluation of the proposed models on public ofﬂine datasets\\nand propose two ways to evaluate the models, which are the\\nofﬂine evaluation and the online evaluation.\\n1) Ofﬂine evaluation: Intuitively, the ofﬂine evaluation of\\nthe trained models is to test the recommendation performance\\nwith the learned policy, which is described in Algorithm 2.\\nSpeciﬁcally, for a given session Sj, the recommender only\\nrecommends the items that appear in this session, denoted as\\nI(Sj), rather than the ones in the whole item space. The reason\\nis that we only have the ground truth feedback for the items in\\nthe session in the recoreded ofﬂine log. For each timestep, the\\nrecommender agent takes an action ataccording to the learned\\npolicyπθ, and recommends an item it∈I(Sj)based on the\\nactionatby Eq. (2) (lines 4-5). After that, the recommender\\nobserves the reward rt=R(st,at)according to the feedback\\nof the recommended item itby Eq. (10) (lines 5-6). Then the\\nuser state is updated to st+1and the recommended item itis\\nremoved from the candidate set I(Sj)(lines 7-8). The ofﬂine\\nevaluation procedure can be treated as a rerank procedure of\\nthe candidate set by iteratively selecting an item w.r.t. the\\naction generated by the Actor network in DRR framework.\\nMoreover, the model parameters are not updated in the ofﬂine\\nevaluation.\\nAlgorithm 2: Ofﬂine Evaluation Algorithm of DRR\\nFramework\\ninput : state window size nand reward function R\\n1Observe the initial state s0and item setIaccording to\\nthe ofﬂine log\\n2for t = 1, T do\\n3 Observe current state st={i1,...,in}\\n4 Execute action at=πθ(st)according to the current\\npolicy\\n5 Observe the recommended item itaccording to\\nactionatby Eq. (2)\\n6 Get rewardrt=R(st,at)from the feedback located\\nin the users’ log by Eq. (10)\\n7 Update to a new state st+1=f(Ht+1), where\\nHt+1={i2,...,in,it}ifrtis positive, otherwise,\\nHt+1=Ht\\n8 removeitfromI\\n2) Online evaluation with environment simulator: As afore-\\nmentioned that it is risky and costly to directly deploy the\\nRL based models on recommender systems. Therefore, we\\nconduct online evaluation with an environment simulator. In\\nthis paper, we pretrain a PMF [37] model as the environmentsimulator, i.e., to predict an item’s feedback that the user\\nnever rates before. The online evaluation procedure follows\\nAlgorithm 1, i.e., the parameters continuously update dur-\\ning the online evaluation stage. Its major difference from\\nAlgorithm 1 is that the feedback of a recommended item\\nis observed by the environment simulator. Moreover, before\\neach recommendation session starting in the simulated online\\nevaluation, we reset the parameters back to θandωwhich is\\nthe policy learned in the training stage for a fair comparison.\\nV. E XPERIMENT\\nA. Datasets and Evaluation Metrics\\nWe adopt the following publicly available datasets from the\\nreal world to conduct the experiments:\\n•MovieLens (100k)4. A benchmark dataset comprises\\nof 0.1 million ratings from users to the recommended\\nmovies on MovieLens website.\\n•Yahoo! Music (R3)5. This dataset contains over 0.36\\nmillion ratings of songs collected from two different\\nsources. The ﬁrst source consists of ratings provided\\nby users during normal interactions with Yahoo! Music\\nservices. The second source consists of ratings of ran-\\ndomly selected songs collected during an online survey\\nby Yahoo! Research. We normalize the ratings to discrete\\nvalues from 1 to 5.\\n•MovieLens (1M)6. A benchmark dataset includes of 1\\nmillion ratings from the MovieLens website.\\n•Jester (2)7. This dataset contains over 1.7 million real-\\nvalue ratings (-10.0 to +10.0) over jokes in an online joke\\nrecommender system.\\nNote that except for Jester, the ratings in the other datasets\\nare discrete values from 1 to 5, and the statistic information\\nof the datasets is given in Table I. The MovieLens (100k) and\\nMovieLens (1M) are abbreviated as ML (100k) and ML (1M)\\nrespectively.\\nTABLE I\\nSTATISTIC INFORMATION OF THE DATASETS\\nML (100k) Yahoo! Music ML (1M) Jester\\n# user 943 15,400 6,040 63,978\\n# item 1,682 1,000 3,952 150\\n# ratings 100,000 365,740 1,000,209 1,761,439\\nWe conduct both ofﬂine and simulated online evaluation\\non these four datasets. For the ofﬂine evaluation, we utilize\\nPrecision@k and NDCG@k as the metrics to measure the\\nperformance of the proposed models. For the simulated online\\nevaluation, we leverage the total accumulated rewards as the\\nmetric.\\n4https://grouplens.org/datasets/movielens/100k/\\n5https://webscope.sandbox.yahoo.com/\\n6https://grouplens.org/datasets/movielens/1m/\\n7http://eigentaste.berkeley.edu/dataset/'),\n",
       " Document(page_content='B. Compared Methods\\nWe compare the proposed methods with some representative\\nbaseline methods. For the ofﬂine evaluation, we compare to\\nconventional methods including Popularity, PMF [37] and\\nSVD++ [38], and a RL based method DRR-n. Moreover, the\\nonline evaluation baselines contain the state-of-the-art multi-\\narmed bandits methods LinUCB [39] and HLinUCB [40] and\\nthe DRR-n as well.\\n•Popularity recommends the most popular item, i.e., the\\nitem with the highest average rating or the items with\\nlargest number of positive ratings8from current available\\nitems to the users at each timestep.\\n•PMF makes a matrix decomposition as SVD, while it\\nonly takes into account the non zero elements.\\n•SVD++ mixes strengths of the latent model as well as\\nthe neighborhood model.\\n•LinUCB selects an arm (item) according to the estimated\\nupper conﬁdence bound of the potential reward.\\n•HLinUCB further learns hidden features for each arm to\\nmodel the potential reward.\\n•DRR-n simply utilizes the concatenation of the item\\nembeddings to represent user state, which is widely\\nused in previous studies. Although it is under the DRR\\nframework, we treat this method as a baseline to assess\\nthe effectiveness of our proposed state representation\\nmodule.\\nC. Experimental Settings\\nFor each dataset, we choose 80% of the interactions in\\neach user session as the training set, and leave the rest as the\\ntesting set. Moreover, for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), the positive ratings are 4and5, while for\\nJester, the positive ones are those higher than 0. The number\\nof latest positively rated items n, which is empirically set to 5.\\nWe perform PMF to pretrain the 100-dimensional embeddings\\nof the users and items. Moreover, in each episode, we do not\\nrecommend repeated items, i.e., we remove the ones already\\nrecommended from the candidate set. The discount rate γis\\n0.9. We utilize Adam optimizer for all the RL based methods\\nwithL2-norm regularization to prevent overﬁtting. As for the\\nreward function, we empirically normalize the ratings into\\nrange [-1 ,1] and utilize the normalized ones as the feedback of\\nthe corresponding recommendations. For instance, in timestep\\nt, the recommender agent recommends an item jto useri,\\n(denoted as action ain states), and the rating ratei,jcomes\\nfrom the interaction logs if user iactually rates item j, or\\nfrom a predicted value by the simulator otherwise. Therefore,\\nthe reward function can be deﬁned as follows:\\nR(s,a) =1\\n2(ratei,j−3)\\nR(s,a) =ratei,j/10(10)\\n8To get a better result of popularity based recommendation, we both test\\nthe two strategies, and choose the best one to report.where the ﬁrst setting is for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), and the second one is for Jester. All the\\nbaseline methods are carefully tuned for a fair comparison.\\nWe model the recommendation procedure as an interaction\\nepisode with length T, and the hyper-parameter Tis tuned\\nfor different datasets (detailed in Section V .E).\\nD. Results and Analysis\\n1) Ofﬂine Evaluation Results and Analysis: The ofﬂine\\nevaluation results are summarized from Table II to Table V\\nrespectively, where the best results are marked in bold type.\\nIn the ofﬂine evaluation, we compare the proposed methods\\nto some representative ofﬂine learning methods. The results\\nsuggest that the proposed methods under the DRR framework\\noutperform the baselines on most of datasets, which demon-\\nstrates the effectiveness of our proposed methods.\\nSpeciﬁcally, as aforementioned that, we propose three dif-\\nferent network structure in the state representation module to\\nmodel the explicit interactions of the users and items under the\\nDRR framework, which are the DRR-p, DRR-u and DRR-\\nave. From the results in Table II to Table V, we ﬁnd that\\nthe three methods all outperform the baselines in most cases.\\nMoreover, DRR-n that simply concats the item embeddings\\nto represent the state s, performs worse than the proposed\\nDRR-p, DRR-u and DRR-ave. From the observations, we\\ncan conclude in two folds: (i) the proposed methods indeed\\nhave the capability of long-term scheduling and dynamic\\nadaptation, which are ignored by conventional methods; (ii)\\nthe proposed state representation module well captures the\\ndynamic interactions between the users and items, and the\\nstate should not be simply concatenate with fully connected\\nlayers as DRR-n does, which may result in information loss.\\nCompared with DRR-p, DRR-u and DRR-ave, we can see\\nthat DRR-ave outperforms DRR-u, and DRR-u is superior than\\nDRR-p on the four datasets in most cases. The reasons are as\\nfollows: 1) The DRR-u method has better performance than\\nDRR-p, because DRR-u only captures the interactions of user’s\\nhistorical items, but also seizes the personalization information\\nthrough the user-item interactions. 2) DRR-ave performs the\\nbest, because of two reasons: (i) DRR-ave method captures\\nthe personalization information through user-item interactions;\\n(ii) as noted in Section IV , by using the average pooling, it\\neliminates the position effects in H.\\nTABLE II\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (100 K)DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6933 0.6012 0.9104 0.9008\\nPMF 0.6988 0.6194 0.9095 0.8968\\nSVD++ 0.7034 0.6255 0.9125 0.8991\\nDRR-n 0.7185 0.6387 0.9147 0.9004\\nDRR-p 0.7263 0.6448 0.9076 0.9015\\nDRR-u 0.7417 0.6536 0.9183 0.9062\\nDRR-ave 0.7887 0.6935 0.9255 0.9046'),\n",
       " Document(page_content='items, and the items consumed by similar users tend to have\\nthe same rating. However, conventional CF based methods\\ntend to suffer from the data scarcity, because the similarity\\ncalculated from sparse data can be very unreliable. Matrix\\nfactorization (MF), as an advanced CF technique, plays an\\nimportant role in recommender systems. MF models [2]–[5]\\ncharacterize both items and users by vectors in the same space,\\nwhich are inferred from the observed user-item interactions.\\nRegarding the recommendation as a binary classiﬁcation prob-\\nlem, logistic regression and its variants [6] are also utilized\\nin recommender systems. However, logistic regression based\\nmodels are hard to generalize to the feature interactions that\\nnever or rarely appear in the training data. Factorization\\nmachines [7] model pairwise feature interactions as inner\\nproduct of latent vectors between features and show promising\\nresults. As an extension to FM, Field-aware FM (FFM [8])\\nenables each feature to have multiple latent vectors to interact\\nwith different ﬁelds. Recently, deep learning models [9]–[12]\\nare applied to model the complicated feature interactions for\\nrecommendation.\\nAs a distinguished direction, contextual multi-armed bandits\\nare also utilized to model the interactive nature of recom-\\nmender systems [13]–[17]. Li et al. apply Thompson Sampling\\n(TS) and Upper Conﬁdent Bound (UCB) to balance the trade-\\noff between exploration and exploitation in [13] and [14],\\nrespectively. The authors of [16] propose a dynamic context\\ndrift model to address the time varying problem. To integrate\\nthe latent vectors of items and users with some exploration,\\nthe authors of [15], [17] combine matrix factorization with\\nmulti-armed bandits.\\nHowever, all these methods suffer from two limitations.\\nFirst, they consider the recommendation procedure as a static\\nprocess, i.e., they assume the underlying user’s preference\\nkeeps static and they aim to learn the user’s preference as\\nprecise as possible. Second, they are learned to maximize the\\nimmediate rewards of recommendations, but ignore the long-\\nterm beneﬁts that the recommendations can make.\\nB. RL based Recommendation Techniques\\nAs model-based RL techniques [18], [27] are inapplicable\\nin recommendation scenario due to their high time complex-\\nity, most researchers turn to model-free RL techniques. The\\nmodel-free RL techniques can be divide into two categories:\\npolicy-based and value-based.\\nPolicy-based approaches [28], [32], [33] aim to generate\\na policy, of which the input is a state, and the output is\\nan action. These works apply deterministic policies, which\\ngenerates an action directly. Dulac-Arnold et al. [33] resolves\\nthe large action space problem by modeling the state in a\\ncontinuous item embedding space and selecting the items via\\na neighborhood method. However, as the underlying algorithm\\nis essentially a continuous-action algorithm, its performance\\nmay be cursed by the gap between the continuous and discrete\\naction spaces. In [28], [32], the policy network outputs a\\ncontinuous action representation, and the recommendation is\\ngenerated by ranking the items with their scores, which arecomputed by a pre-deﬁned function with the action representa-\\ntion and the item embeddings as input. However, one common\\nlimitation of the studies is that they do not carefully learn the\\nstate representation.\\nFor value-based approaches [19], [29], the action with max-\\nimum Q-value over all the possible actions is selected as the\\nbest action. Zhao et al. [29] take both user’s positive feedback\\nand negative feedback into consideration when modeling user\\nstate. Dueling Q-network is utilized in [19], to model Q-\\nvalue of a state-action pair. Moreover, a minor update with\\nexploration by dueling bandit gradient descent is proposed.\\nHowever, such value-based approaches need to evaluate the\\nQ-values of all the actions under a speciﬁc state, which is\\nvery inefﬁcient when the number of actions is large.\\nTo make RL based recommendation techniques suitable for\\nlarge-scale scenario, in this paper, we propose the DRR frame-\\nwork which carefully and explicitly model the interactions\\nbetween users and items to learn the state representation.\\nIII. P RELIMINARIES\\nThe essential underlying model of reinforcement learning\\nis Markov Decision Process (MDP). An MDP is deﬁned as\\n(S,A,P,R,γ).Sis the state space and Ais the action\\nspace.P:S×A×S ↦→ [0,1]is the state transition\\nfunction.R:S×A×S ↦→ Ris the reward function. γ\\nis the discount rate. The objective of an agent in an MDP\\nis to ﬁnd an optimal policy ( πθ:S×A ↦→ [0,1]) which\\nmaximizes the expected cumulative rewards from any state\\ns∈ S , i.e.,V∗(s) = max πθEπθ{∑∞\\nk=0γkrt+k|st=s},\\nor maximizes equivalently the expected cumulative rewards\\nfrom any state-action pair s∈ S,a∈ A , i.e.,Q∗(s,a) =\\nmaxπθEπθ{∑∞\\nk=0γkrt+k|st=s,at=a}. Here Eπθis the\\nexpectation under policy πθ,tis the current timestep and rt+k\\nis the immediate reward at a future timestep t+k.\\nWe model the recommendation procedure as a sequential\\ndecision making problem, in which the recommender (i.e.,\\nagent) interacts with users (i.e., environment) to suggest a list\\nof items sequentially over the timesteps, by maximizing the\\ncumulative rewards of the whole recommendation procedure.\\nMore speciﬁcally, the recommendation procedure is modeled\\nby an MDP, as follows.\\n•StatesS.A statesis the representation of user’s positive\\ninteraction history with recommender, as well as her\\ndemographic information (if it exists in the datasets).\\n•ActionsA.An actionais a continuous parameter vector\\ndenoted as a∈R1×k. Each item it∈R1×k1has a\\nranking score, which is deﬁned as the inner product of\\nthe action and the item embedding, i.e., ita⊤. Then the\\ntop ranked ones will be recommended.\\n•TransitionsP.The state is modeled as the representa-\\ntion of user’s positive interaction history. Hence, once\\nthe user’s feedback is collected, the state transition is\\ndetermined.\\n1itis the embedding of item i, which can be generated by MF or V AE.'),\n",
       " Document(page_content='TABLE III\\nPERFORMANCE COMPARISON OF ALL METHODS ON YAHOO ! M USIC\\nDATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.3826 0.3805 0.8870 0.8811\\nPMF 0.3835 0.3817 0.8837 0.8802\\nSVD++ 0.3857 0.3821 0.8887 0.8813\\nDRR-n 0.3844 0.3819 0.8876 0.8810\\nDRR-p 0.3850 0.3822 0.8883 0.8815\\nDRR-u 0.3864 0.3827 0.8889 0.8819\\nDRR-ave 0.3917 0.3839 0.9004 0.8949\\nTABLE IV\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (1M) DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.7141 0.6181 0.8906 0.8738\\nPMF 0.7072 0.6193 0.8901 0.8746\\nSVD++ 0.7142 0.6258 0.9009 0.8776\\nDRR-n 0.7151 0.6221 0.8902 0.8751\\nDRR-p 0.7346 0.6366 0.8909 0.8753\\nDRR-u 0.7375 0.6385 0.8912 0.8763\\nDRR-ave 0.7693 0.6594 0.9112 0.8980\\n2) Simulated online evaluation results and analysis: The\\nresults of the simulated online evaluation are summarized in\\nTable VI, where the best results are marked in bold type. In the\\nexperiment, we only compare with the baseline methods that\\ncan perform online learning, which are LinUCB, HLinUCB\\nand DRR-n. Again, we ﬁnd that the proposed methods deliver\\nhigher rewards than all the baselines.\\nOn the one hand, the fact suggests that the proposed\\nRL-based methods model dynamic adaptation and long-term\\nrewards better than the multi-armed bandits based methods\\nLinUCB and HLinUCB. On the other hand, the observation\\nindicates that the proposed state representation structures are\\nsuperior to the naive full-connected network in DRR-n. Again,\\nwe observe that DRR-ave performs the best among all the three\\nproposed interaction modeling structures.\\nE. Parameter Study\\nIn this subsection, we investigate how the episode length\\nTaffect the performance of proposed methods. Figure 7\\nshows the results9. From the left part of Figure 7, we observe\\nthat the performance on MovieLens ﬁrst increases and then\\ndecreases as the length of the episode is gradually increased,\\nand the summit appears at T= 10 . A similar tend can be\\nfound for the Yahoo! Music from the right part of Figure 7,\\nwhere the performance peaks at T= 20 . The reason may\\ndue to the trade-off between the exploitation and exploration.\\nWhen the episode length is small, the user can not fully\\ninteract with the recommender agent, i.e., the exploration is\\ninsufﬁcient. As we enlarge the episodes, the recommender\\n9Due to the space limit, We only present the performance of DRR-ave,\\nwhile DRR-p and DRR-u have similar observationsTABLE V\\nPERFORMANCE COMPARISON OF ALL METHODS ON JESTER DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6167 0.6012 0.8932 0.8703\\nPMF 0.6171 0.6015 0.8740 0.8676\\nSVD++ 0.6184 0.6027 0.8819 0.8614\\nDRR-n 0.6178 0.6021 0.8915 0.8724\\nDRR-p 0.6181 0.6029 0.8934 0.8753\\nDRR-u 0.6217 0.6043 0.8974 0.8805\\nDRR-ave 0.6278 0.6076 0.9124 0.9079\\nTABLE VI\\nTHE REWARDS OF ALL METHODS ON THE FOUR DATASETS .\\nModel ML (100k) Yahoo! Music ML (1M) Jester\\nLinUCB 1,958 30,462.5 30,174 141,358.4\\nHLinUCB 1,475 32,725 32,785.5 147,105.5\\nDRR-n 2,654.5 35,382.5 35,860 165,844.5\\nDRR-p 2,832 37,328.5 36,653 177,414.2\\nDRR-u 2,869 42,174.5 37,615 183,517.6\\nDRR-ave 3,251.5 49,095 40,588 194,860.7\\nagent can explore (interact with users) adequately, i.e., the\\nrecommender agent captures the user’s preference, so that the\\nperformance improves. However, if the episodes are too large,\\nthe recommender focuses on exploiting locally, but the user\\npreferred items is limited, therefore the performance declines\\nas we do not recommend repeated items to user. Hence, we\\nshould nicely trade off the exploration and exploitation by\\nsetting a suitable value for T.\\nFig. 7. Parameter study on episode length Tin MovieLens and Yahoo!Music\\ndatasets\\nF . Case Study\\nIn this subsection, we present an example to show the\\ndifferent recommendation manner between LinUCB and DRR-\\nave on MovieLens dataset. Speciﬁcally, we randomly pick up\\na user with ID 11, and conduct the recommendation procedure\\nwith LinUCB and DRR-ave respectively. To verify the reaction\\nto the same recommendation scenario, we ﬁx the ﬁrst three\\nrecommended items and to see what will happen next. The\\nresults of recommended item and the reward are reported in\\nTable VII.\\nFrom Table VII, we can see that LinUCB and DRR-ave\\nreact differently when given two consecutive negative recom-\\nmendations (Eraser and First Knight). Speciﬁcally, LinUCB')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat implementando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4-1106-preview',\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = docsearch.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Usando o contexto abaixo, responda a pergunta abaixo. Se a pergunta não estiver relacionada ao contexto, esqueça o contexto dado e responda como você normalmente responderia, pois você ainda é um assistente prestativo. E se apesar da pergunta não estiver relacionada você não saber a resposta, apenas diga que não sabe como normalmente você faria.\n",
    "\n",
    "    Contextos:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = [SystemMessage(content='Você é um assistente gentil e prestativo. Você ajudará nos estudos acerca do artigo de aprendizado por reforço para sistemas de recomendação. Busque responder as perguntas quando conseguir!')]\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=augment_prompt(message)))\n",
    "    gpt_response = chat(history_langchain_format)\n",
    "    return gpt_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(predict).launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
