{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Tutorial - From Basic do Advanced\n",
    "\n",
    "The goal of this tutorial is to explore a set of techniques for giving LLMs context over private, recent or specific data in order to avoid LLMs hallucinations or them not being able to give a proper answer.\n",
    "\n",
    "This set of techniques are commonly known as RAG, that stands for Retrieval Augmented Generation.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_**Warning**_ <br>\n",
    "Before diving into the world of RAG, we strongly recommend you, mainly if you are not very familiarized with the field, to read the glossary below. It will give you a basic understanding of fundamental topics regarding Generative AI, Machine Learning and LLMs, crutial in order to better understand RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "#### Machine Learning\n",
    "\n",
    "#### Generative AI\n",
    "\n",
    "#### Large Language Models (LLMs)\n",
    "\n",
    "#### Query\n",
    "\n",
    "#### Hallucinations\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "#### Vector Database\n",
    "\n",
    "#### Semantic Search\n",
    "\n",
    "#### System Message\n",
    "\n",
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech Stack\n",
    "\n",
    "The following libraries and technologies will be used in the development of this tutorial and the applications of RAG within it.\n",
    "\n",
    "* Langchain/Llama Index\n",
    "* Pinecone/MongoDB - Vector Database\n",
    "* LLMs APIs (OpenAI, Anthropic, Claude)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG and how it works?\n",
    "\n",
    "RAG is a technique for giving LLMs context over private, recent or specific data that the large language model had no previous access to. Its goal is to avoid LLMs hallucinations or them not being able to give a proper answer.\n",
    "\n",
    "The process of creating an infrastructure for implementing RAG consists on 4 steps\n",
    "* Getting the data (specific context you want the LLM to know)\n",
    "* Chunking the data (dividing it into small pieces - chunks)\n",
    "* Embedding the chunks (transforming the chunks into 'lots of numbers' - vectors of dimension _n_)\n",
    "* Storing the embeddings in a vector database\n",
    "\n",
    "When this infrastructure is built, the prompting workflow works as below:\n",
    "\n",
    "(IMAGE SHOWING BASIC RAG WORKFLOW)\n",
    "\n",
    "As shown, the process has 3 main stages:\n",
    "* Embedding\n",
    "* Retrieval\n",
    "* Generation\n",
    "\n",
    "That workflow is showing the most simple process a RAG application undertakes, in which the user prompt (query) is embedded and then a similairty search is conducted in order to find the chunks that are more related to the prompt made. After that, the most related chunks (top-k chunks) are added to the user prompt and fed into the LLM. The LLM now have (hopefully) not just the user prompt but the necessary context to answer it properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application - Doing xyzxyzxyzxyz\n",
    "Now we will conduct an application of the RAG technique, starting with a simple RAG application and exploring more advanced approaches later on. The goal of this example is to explore the different variables of RAG infrastructure and how they might affect the quality of the answer provided by the LLM.\n",
    "\n",
    "Different (i) embedding models, (ii) vector databases, (iii) similarity searches and (iv) retrieval techniques will be explored.\n",
    "\n",
    "At the end of the day, our main goal when applying RAG it to increase the assertiviness and quality of the document that is retrieved as context, in order to give the LLM enough information to provide a proper answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs pra mim: Testar pelo menos duas formas de embeddar, duas bases de dados (pensar se esse aq é válido), duas formas de calcular similaridade, duas formas de retrieve contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Augmentation (Adding most similar Context to User Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example application 2 - (Exploring Different Advanced Technique)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
